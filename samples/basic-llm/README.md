# Sample: Basic LLM on Web

This is a sample project that demonstrates how to run a basic local language model (LLM) like Gemma 2B on the web using WebAssembly. This project is a part of the [Web Edge AI](https://ai.google.dev/edge) initiative, which aims to provide developers with easy-to-use tools for integrating AI capabilities into web applications without relying on external servers, ensuring faster response times and enhanced privacy. For details visit [LLM Inference guide for Web](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js).

## Setup

- Clone the repository
- Download `gemma-2b-it-gpu-int4.bin` from [GEMMA 2B](https://www.kaggle.com/models/google/gemma/tfLite/) and place it in the `samples/basic-llm` directory
- Run a local server in the `samples/basic-llm` directory
